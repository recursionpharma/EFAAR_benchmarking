{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efaar_benchmarking.data_loading import *\n",
    "from efaar_benchmarking.efaar import *\n",
    "from efaar_benchmarking.constants import *\n",
    "from efaar_benchmarking.benchmarking import *\n",
    "from efaar_benchmarking.plotting import *\n",
    "import pickle\n",
    "\n",
    "pc_count = 128 # embedding size\n",
    "compute_univariate_metrics = False # note that if you change this to True, the run will take a couple hours to complete\n",
    "save_results = True # if True, save computationally expensive results like the univariate metrics to disk not to recompute them next time\n",
    "res_folder = 'data' # folder to save the proccessed data and results in\n",
    "expression_data_folder = '../efaar_benchmarking/expression_data' # folder where the gene expression data is stored, this is needed to filter out unexpressed genes prior to saving the aggregated map for further evaluation\n",
    "\n",
    "recall_threshold_pairs = []\n",
    "start = 0.01\n",
    "end = 0.99\n",
    "step = 0.01\n",
    "while start <= .105 and end >= .895:\n",
    "    recall_threshold_pairs.append((round(start,2), round(end,2)))\n",
    "    start += step\n",
    "    end -= step\n",
    "print(recall_threshold_pairs)\n",
    "\n",
    "all_embeddings_pre_agg = {}\n",
    "\n",
    "dataset = \"JUMP\" # replace accordingly to run the other datasets\n",
    "\n",
    "if dataset == \"GWPS\":\n",
    "    adata_norm = load_gwps(\"normalized\")\n",
    "    adata_raw = load_gwps(\"raw\")\n",
    "    metadata = adata_raw.obs # use adata_raw for both PCA and scVI maps since adata_raw has the same metadata as adata_norm\n",
    "\n",
    "    pert_colname = GWPS_PERT_LABEL_COL\n",
    "    ctrl_colname = GWPS_CONTROL_PERT_LABEL\n",
    "    batch_colname = GWPS_BATCH_COL\n",
    "\n",
    "    embeddings = embed_by_pca_anndata(adata_norm, pc_count)\n",
    "    del adata_norm\n",
    "    all_embeddings_pre_agg[f\"PCA{pc_count}-CS\"] = centerscale_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname)\n",
    "\n",
    "    embeddings = embed_by_scvi_anndata(adata_raw.obs, n_latent=pc_count, n_hidden=pc_count*2)\n",
    "    del adata_raw\n",
    "    all_embeddings_pre_agg[f\"scVI{pc_count}-CS\"] = centerscale_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname)\n",
    "    all_embeddings_pre_agg[f\"scVI{pc_count}-TVN\"] = tvn_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname, batch_col=batch_colname)\n",
    "\n",
    "    unused_keys = []\n",
    "\n",
    "elif dataset == \"JUMP\":\n",
    "    features, metadata = load_cpg16_crispr()\n",
    "    features, metadata = filter_cell_profiler_features(features, metadata)\n",
    "\n",
    "    pert_colname = JUMP_PERT_LABEL_COL\n",
    "    ctrl_colname = JUMP_CONTROL_PERT_LABEL\n",
    "    batch_colname = JUMP_BATCH_COL\n",
    "    batch_colname_2 = JUMP_BATCH_COL_2\n",
    "\n",
    "    print(\"Computing PCA embedding for\", pc_count, \"dimensions...\")\n",
    "    embeddings = embed_by_pca(features.values, metadata, variance_or_ncomp=pc_count, batch_col=batch_colname)\n",
    "    all_embeddings_pre_agg[f\"CP-PCA{pc_count}-CS\"] = centerscale_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname) ## CS alignment\n",
    "    all_embeddings_pre_agg[f\"CP-PCA{pc_count}-TVN\"] = tvn_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname, batch_col_coral=batch_colname_2)  ## TVN alignment\n",
    "\n",
    "    unused_keys = ['negCtrl', 'no-guide']\n",
    "\n",
    "elif dataset == \"PERISCOPE\":\n",
    "    features, metadata = load_periscope()\n",
    "\n",
    "    pert_colname = PERISCOPE_PERT_LABEL_COL\n",
    "    ctrl_colname = PERISCOPE_CONTROL_PERT_LABEL\n",
    "    batch_colname = PERISCOPE_BATCH_COL\n",
    "\n",
    "    print(\"Computing PCA embedding for\", pc_count, \"dimensions...\")\n",
    "    embeddings = embed_by_pca(features.values, metadata, variance_or_ncomp=pc_count, batch_col=batch_colname)\n",
    "    all_embeddings_pre_agg[f\"CP-PCA{pc_count}-CS\"] = centerscale_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname) ## CS alignment\n",
    "    all_embeddings_pre_agg[f\"CP-PCA{pc_count}-TVN\"] = tvn_on_controls(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname, batch_col_coral=batch_colname)  ## TVN alignment\n",
    "\n",
    "    unused_keys = ['negCtrl']\n",
    "\n",
    "if save_results:\n",
    "    metadata.to_pickle(f'{res_folder}/{dataset}_pre_agg_metadata.pkl') # save metadata to disk to get the statistics prior to aggregation, we will use it to check the total and expressed gene counts\n",
    "\n",
    "### Compute benchmarks (perturbation signal benchmarks are computed pre-aggregation, while known relationship benchmarks are computed post-aggregation)\n",
    "known_relationship_metrics = {}\n",
    "for k, embeddings in all_embeddings_pre_agg.items():\n",
    "    if compute_univariate_metrics:\n",
    "        dist_res = pert_signal_distance_benchmark(embeddings, metadata, pert_col=pert_colname, batch_col=batch_colname, control_key=ctrl_colname, keys_to_drop=unused_keys, n_samples=1000)\n",
    "        print(k, sum(dist_res.pval <= .01) / sum(~pd.isna(dist_res.pval)))\n",
    "        \n",
    "        cons_res = pert_signal_consistency_benchmark(embeddings, metadata, pert_col=pert_colname, batch_col=batch_colname, keys_to_drop=[ctrl_colname]+unused_keys, n_samples=1000)\n",
    "        print(k, sum(cons_res.pval <= .01) / sum(~pd.isna(cons_res.pval)))\n",
    "\n",
    "        if save_results:\n",
    "            dist_res.to_csv(f'{res_folder}/{dataset}_distance_results_{k}.csv', index=False)\n",
    "            cons_res.to_csv(f'{res_folder}/{dataset}_consistency_results_{k}.csv', index=False)\n",
    "\n",
    "    map_data = aggregate(embeddings, metadata, pert_col=pert_colname, control_key=ctrl_colname)\n",
    "    metrics = known_relationship_benchmark(map_data, recall_thr_pairs=recall_threshold_pairs, pert_col=pert_colname, n_null_samples = 10000, n_iterations = 1)\n",
    "    print(k)\n",
    "    print((metrics.groupby('source')['recall_0.05_0.95'].mean() * 100).round(1))\n",
    "    known_relationship_metrics[f\"{dataset} {k}\"] = metrics\n",
    "\n",
    "    if save_results:\n",
    "        if dataset == \"PERISCOPE\":\n",
    "            expr = pd.read_csv(f'{expression_data_folder}/HeLa_expression.csv') # note that we assume the HeLa expression data was used for PERISCOPE which is the default option in load_periscope()\n",
    "            expr.columns = ['gene', 'tpm']\n",
    "            expr.gene = expr.gene.apply(lambda x: x.split(' ')[0])\n",
    "            expr_genes = list(expr.loc[expr.tpm > 0, 'gene'])\n",
    "            ind = metadata[pert_colname].isin(expr_genes + [ctrl_colname])\n",
    "        elif dataset == \"JUMP\":\n",
    "            expr = pd.read_csv(f'{expression_data_folder}/U2OS_expression.csv', index_col=0)\n",
    "            expr = expr.groupby('gene').zfpkm.agg('median').reset_index()\n",
    "            expr_genes = list(expr.loc[expr.zfpkm >= -3, 'gene'])\n",
    "            ind = metadata[pert_colname].isin(expr_genes + [ctrl_colname])\n",
    "        else:\n",
    "            ind = [True] * len(metadata)\n",
    "        map_data = aggregate(embeddings[ind], metadata[ind], pert_col=pert_colname, control_key=ctrl_colname)\n",
    "        with open(f'{res_folder}/{dataset}_aggr_{k}_map.pkl', 'wb') as outfile:\n",
    "            pickle.dump(map_data, outfile)\n",
    "\n",
    "plot_recall(known_relationship_metrics)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eben",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
